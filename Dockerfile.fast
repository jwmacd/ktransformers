ARG UBUNTU_VERSION=22.04
# This needs to generally match the container host's environment.
ARG CUDA_VERSION=12.1
# Target the CUDA build image
ARG PYTORCH_VERSION=2.5.1
ARG CUDNN_VERSION=9
ARG BASE_CUDA_DEV_CONTAINER=pytorch/pytorch:${PYTORCH_VERSION}-cuda${CUDA_VERSION}-cudnn${CUDNN_VERSION}-devel
ARG BASE_CUDA_RUN_CONTAINER=pytorch/pytorch:${PYTORCH_VERSION}-cuda${CUDA_VERSION}-cudnn${CUDNN_VERSION}-runtime

# ========== BUILD ==========
FROM ${BASE_CUDA_DEV_CONTAINER} as builder

# Hardcode optimizations instead of using build args
ENV CPU_INSTRUCT=FANCY
# Set CUDA architecture for RTX 4090 and 3090 GPUs
ENV CUDA_DOCKER_ARCH="8.6;8.9"

# Set working directory and CUDA path
WORKDIR /workspace
ENV CUDA_HOME=/usr/local/cuda
ENV CUDA_DOCKER_ARCH=${CUDA_DOCKER_ARCH}

# Install dependencies
RUN apt update -y && apt install -y --no-install-recommends \
    libtbb-dev \
    libssl-dev \
    libcurl4-openssl-dev \
    libaio1 \
    libaio-dev \
    libfmt-dev \
    libgflags-dev \
    zlib1g-dev \
    patchelf \
    git \
    wget \
    vim \
    gcc \
    g++ \
    cmake \
    libgomp1 \
    curl && \
    rm -rf /var/lib/apt/lists/*

# Clone the repository
RUN git clone https://github.com/kvcache-ai/ktransformers.git

# Enter project directory and initialize submodules
WORKDIR /workspace/ktransformers
RUN git submodule update --init --recursive

# Copy local files to override cloned repository (ensures we use our local changes)
COPY . .

# Upgrade pip and install dependencies with version pinning
RUN pip install --upgrade pip && \
    pip install ninja==1.11.1 pyproject==1.3.1 numpy==1.26.0 cpufeature==0.2.0 aiohttp==3.9.1 zmq==0.0.0 openai==1.6.1

# Use pre-built flash-attn wheel (much faster) if available
COPY saved_wheels /workspace/saved_wheels/
RUN if [ -n "$(find /workspace/saved_wheels -name 'flash_attn*.whl' -type f 2>/dev/null)" ]; then \
        echo "Using pre-built flash-attn wheel" && \
        pip install /workspace/saved_wheels/flash_attn*.whl; \
    else \
        echo "No pre-built wheel found, installing from source (this will be slow)" && \
        pip install flash-attn==2.5.5 && \
        mkdir -p /workspace/saved_wheels && \
        find /opt/conda -name "flash_attn*.whl" -type f -exec cp {} /workspace/saved_wheels/ \; || echo "No wheel found"; \
    fi

# Install flashinfer for longer context support
RUN pip install git+https://github.com/flashinfer-ai/flashinfer.git

# Install ktransformers with multi-GPU support and specific CUDA architectures
RUN USE_BALANCE_SERVE=1 \
    KTRANSFORMERS_FORCE_BUILD=TRUE \
    TORCH_CUDA_ARCH_LIST=${CUDA_DOCKER_ARCH} \
    pip install . --no-build-isolation --verbose

RUN pip install third_party/custom_flashinfer/

# Copy C++ runtime libraries
RUN cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 /opt/conda/lib/

# Clean pip cache
RUN pip cache purge

# Create a directory for models
RUN mkdir -p /workspace/models

# ========== RUNTIME ==========
FROM ${BASE_CUDA_RUN_CONTAINER}

WORKDIR /workspace
COPY --from=builder /opt/conda /opt/conda
COPY --from=builder /workspace/ktransformers /workspace/ktransformers
COPY --from=builder /workspace/saved_wheels /workspace/saved_wheels

# Create directories for configs and models
RUN mkdir -p /workspace/models /workspace/configs

# Set environment variables for multi-GPU and performance
ENV NVIDIA_VISIBLE_DEVICES=all
ENV OMP_NUM_THREADS=32
ENV MKL_NUM_THREADS=32
ENV KMP_AFFINITY=granularity=fine,compact,1,0

# Add healthcheck
HEALTHCHECK --interval=30s --timeout=30s --start-period=30s --retries=3 \
    CMD curl -f http://localhost:10002/v1/models || exit 1

# Set entry point to run the server
WORKDIR /workspace/ktransformers
ENTRYPOINT ["python", "-m", "ktransformers.server.main"]