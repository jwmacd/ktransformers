log:
  dir: "logs"
  file: "ktransformers.log"
  level: "info"
  backup_count: 5

server:
  ip: 0.0.0.0
  port: 10002

db:
  type: "sqllite"
  database: "server.db"
  host: "./"
  pool_size: 10

user:
  secret_key: "981f1dd2a44e27d68759d0252a486568ed43480b4e616a26e3af3709c3a7ce73"
  algorithm: "HS256"

model:
  type: balance_serve  # Enable load balancing across multiple GPUs
  name: DeepSeek-V3
  path: /workspace/models/DeepSeek-V3
  gguf_path: /workspace/models/DeepSeek-V3-GGUF
  device: cuda:0,1,2,3,4  # Using all 5 GPUs (4x RTX 4090 + 1x RTX 3090)
  cache_lens: 16384  # Larger cache for memory efficiency
  max_new_tokens: 2048  # Allow longer responses
  top_k: 50
  top_p: 0.8
  temperature: 0.7
  repetition_penalty: 1.1
  batch_size: 4  # Utilize multiple GPUs effectively

web:
  mount: False
  open_cross_domain: True

ext:
  cpu_infer: 32  # Match your 32-core CPU

long_context:
  max_seq_len: 32000
  block_size: 128
  local_windows_len: 4096
  second_select_num: 32
  anchor_type: DYNAMIC
  kv_type: FP16
  dense_layer_num: 2
  anchor_num: 1
  preselect_block: True
  head_select_mode: SHARED
  preselect_block_count: 32
  layer_step: 1
  token_step: 

async_server:
  sched_strategy: "FCFS"
  sched_port: 56441
  sched_metrics_port: 54321
  kvc2_metrics_port: 54391
  max_batch_size: 4  # decode count + prefill count, in one mini batch

attn:
  page_size: 256
  chunk_size: 256

kvc2:
  gpu_only: true
  utilization_percentage: 0.9  # Leave a little headroom
  cpu_memory_size_GB: 380  # Almost all of your 384GB RAM 